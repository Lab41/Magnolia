{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Forked Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Standard python libraries\n",
    "import numpy as np\n",
    "import time\n",
    "import sys\n",
    "import matplotlib.pylab as plt\n",
    "import functools\n",
    "%matplotlib inline\n",
    "\n",
    "## Magnolia data iteration\n",
    "sys.path.append('../../')\n",
    "from src.features.mixer import FeatureMixer\n",
    "from src.features.wav_iterator import batcher\n",
    "from supervised_iterator_experiment import SupervisedIterator, SupervisedMixer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.allow_growth = True\n",
    "sess = tf.Session(config=config)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "numsources = 2\n",
    "batchsize = 256\n",
    "datashape = (40, 257)\n",
    "embedding_size = 600\n",
    "restore_session=False\n",
    "libridev='/local_data/teams/magnolia/libri-dev.h5'\n",
    "libritrain='/local_data/teams/magnolia/librispeech/processed_train-clean-100.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a supervised mixer and batcher\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if numsources == 3:\n",
    "    mixer = SupervisedMixer([libritrain,libritrain,libritrain], shape=datashape, \n",
    "                         mix_method='add', diffseed=True, return_key=True)\n",
    "else:\n",
    "    mixer = SupervisedMixer([libritrain,libritrain], shape=datashape, \n",
    "                            mix_method='add', diffseed=True, return_key=True)\n",
    "\n",
    "\n",
    "# Check the time\n",
    "tbeg = time.clock()\n",
    "X, Y, I = mixer.get_batch(batchsize)\n",
    "tend = time.clock()\n",
    "print('Supervised feature mixer with 3 libridev sources timed at ', (tend-tbeg), 'sec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NEURAL NETWORK\n",
    "\n",
    "The lost function takes in as input the variable `Vlast` for last layer ($V_{last}$, where a vector in $V_{last}$ is $v_{l}$). (That's the first couplet lines, where one just makes a tensorflow variable `Vlasttf`.)\n",
    "\n",
    "The actual cost function is the *word2vec* objective function, where samples are positively and negatively sampled and then mixed. Let $A$ be a matrix of \"attractors\", so to speak. (We'll not use that terminology later on.) Then a positively sampled vector $a_p$ and a few negatively sampled ones $a_{n_1}$ and $a_{n_2}$ are all columns in $A$. The loss over a batch $B$ is denoted `tfbatchlo`, and is specified as:\n",
    "\n",
    "$$ \\mathcal{L}(v_{last}) = \\log \\sigma ( v_l^T a_p) + \\sum_j \\log \\sigma( -1 \\cdot v_l^T a_{n_j} )$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def scope(function):\n",
    "    attribute = '_cache_' + function.__name__\n",
    "    name = function.__name__\n",
    "\n",
    "    @property\n",
    "    @functools.wraps(function)\n",
    "    def decorator(self):\n",
    "        if not hasattr(self,attribute):\n",
    "            with tf.device(\"/gpu:0\"):\n",
    "                with tf.variable_scope(name):\n",
    "                    setattr(self,attribute,function(self))\n",
    "        return getattr(self,attribute)\n",
    "    \n",
    "    return decorator\n",
    "\n",
    "class L41Convolve:\n",
    "    def __init__(self, X, Y, F, I, layer_size, embedding_size, num_labels):\n",
    "        \n",
    "#         self.Vclass = tf.Variable(tf.random_normal( [embedding_size, num_labels, F], stddev=0.08 ), \n",
    "#                                   dtype=tf.float32,\n",
    "#                                   name = 'Vclass')\n",
    "        self.Vclass = tf.Variable(tf.random_normal( [embedding_size, num_labels, 1], stddev=0.05), \n",
    "                                  dtype=tf.float32, name='Vclass')\n",
    "        \n",
    "        self.X = X\n",
    "        self.Y = Y\n",
    "        \n",
    "        self.F = F\n",
    "        self.I = I\n",
    "        \n",
    "        self.layer_size = layer_size\n",
    "        self.embedding_size = embedding_size\n",
    "                \n",
    "        self.network\n",
    "        self.cost\n",
    "        self.optimizer\n",
    "        \n",
    "    \n",
    "    def weight_variable(self,shape):\n",
    "        initial = tf.truncated_normal(shape, stddev=tf.sqrt(2.0/shape[0]))\n",
    "        return tf.Variable(initial)\n",
    "    \n",
    "    def conv1d(self,x, W):\n",
    "        return tf.nn.conv1d(x, W, stride=1, padding='SAME')\n",
    "    \n",
    "    def conv1d_layer(self,in_layer,shape):\n",
    "        weights = self.weight_variable(shape)\n",
    "        biases = self.weight_variable([shape[-1]])\n",
    "        \n",
    "        return self.conv1d(in_layer,weights) + biases\n",
    "    \n",
    "    def BLSTM(self, X, size, scope):\n",
    "        forward_input = X\n",
    "        backward_input = tf.reverse(X, [1])\n",
    "        \n",
    "        with tf.variable_scope('forward_' + scope):\n",
    "            forward_lstm = tf.contrib.rnn.BasicLSTMCell(size//2)\n",
    "            forward_out, f_state = tf.nn.dynamic_rnn(forward_lstm, forward_input, dtype=tf.float32)\n",
    "        \n",
    "        with tf.variable_scope('backward_' + scope):\n",
    "            backward_lstm = tf.contrib.rnn.BasicLSTMCell(size//2)\n",
    "            backward_out, b_state = tf.nn.dynamic_rnn(backward_lstm, backward_input, dtype=tf.float32)\n",
    "        \n",
    "        return tf.concat([forward_out[:,:,:], backward_out[:,::-1,:]], 2)\n",
    "    \n",
    "    @scope\n",
    "    def network(self):\n",
    "        shape = tf.shape(self.X)\n",
    "        \n",
    "        BLSTM_1 = self.BLSTM(self.X, self.layer_size, 'one')\n",
    "        BLSTM_2 = self.BLSTM(BLSTM_1, self.layer_size, 'two')\n",
    "        \n",
    "        feedforward = self.conv1d_layer(BLSTM_2,[1,self.layer_size,self.embedding_size*self.F])\n",
    "        \n",
    "        embedding = tf.reshape(feedforward,[shape[0],shape[1],self.F,self.embedding_size]) \n",
    "        embedding = tf.nn.l2_normalize(embedding,3)\n",
    "        \n",
    "        return embedding\n",
    "    \n",
    "    @scope\n",
    "    def cost(self):        \n",
    "        \n",
    "        Xshape=tf.shape(self.X)\n",
    "        Yshape=tf.shape(self.Y)\n",
    "        \n",
    "        # things that are necessary for the cost function\n",
    "        Vin = self.network\n",
    "        I = tf.expand_dims( self.I, axis=2 )\n",
    "        Y = self.Y\n",
    "        Vclass = self.Vclass\n",
    "        \n",
    "        # l2 normalization\n",
    "        Vclass = tf.nn.l2_normalize(Vclass, 0)\n",
    "        \n",
    "        # gather the appropriate vectors\n",
    "        Vout = tf.gather_nd( tf.transpose(Vclass, perm=[1,2,0]), I )\n",
    "        \n",
    "        # Broadcasted Vi and Vo\n",
    "        Vinbroad = tf.reshape( Vin, [Yshape[0], 1, Yshape[2], Yshape[3], self.embedding_size])\n",
    "        Voutbroad= tf.reshape( Vout, [Yshape[0], Yshape[1], 1, 1, self.embedding_size] )\n",
    "                \n",
    "        # Correlate all the vectors:\n",
    "        lossfxn = - tf.log( tf.nn.sigmoid( Y * tf.reduce_sum(Vinbroad * Voutbroad, 4) ) )\n",
    "        \n",
    "        # Sum correlations over positive and negative correlations\n",
    "        lossfxn = tf.reduce_sum( lossfxn, 1 )\n",
    "        \n",
    "        # Average over all the batches\n",
    "        lossfxn = tf.reduce_mean( lossfxn, 0)\n",
    "        \n",
    "        # To do: put weight by pre-emphasis or gradient confidence\n",
    "        lossfxn = tf.reduce_mean( lossfxn )\n",
    "        \n",
    "        return lossfxn\n",
    "\n",
    "    @scope\n",
    "    def optimizer(self):\n",
    "        opt = tf.train.AdamOptimizer(learning_rate=2.0)\n",
    "        cost = self.cost\n",
    "        return opt.minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "F = 257\n",
    "layer_size=600\n",
    "embedding_size=40\n",
    "X = tf.placeholder(\"float\", [None,None,F])\n",
    "Y = tf.placeholder(\"float\", [None, None,None,F])\n",
    "I = tf.placeholder(dtype=tf.int32)\n",
    "\n",
    "num_labels=251\n",
    "\n",
    "model = L41Convolve(X, Y, F, I, layer_size, embedding_size, num_labels)\n",
    "\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "iterations = []\n",
    "costs = []\n",
    "\n",
    "if restore_session:\n",
    "    saver.restore(sess, '/data/fs4/home/kni/magnolia/models/l41-model-2spkr-1embed.h5')\n",
    "\n",
    "print(\"Initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for iteration in range(1000000):\n",
    "\n",
    "    # Preprocessing\n",
    "    Xdata, Ydata, Idata = mixer.get_batch(batchsize, out_TF=None)    \n",
    "    Xin = np.sqrt( abs(Xdata) )\n",
    "    Xin = (Xin - Xin.min()) / (Xin.max() - Xin.min())\n",
    "\n",
    "    optloss, cost = sess.run([model.optimizer, model.cost], feed_dict={X: Xin, Y:Ydata, I:Idata})\n",
    "    costs += [cost]\n",
    "    sys.stdout.write('\\rIteration '+str(iteration)+', Cost function = '+str(cost))\n",
    "    \n",
    "    if not ((iteration+1) % 1000):\n",
    "        save_path = saver.save(sess, \"/data/fs4/home/kni/magnolia/models/l41-model-2spkr-1embed-broadcast.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def meanfilter(costs):\n",
    "    return np.convolve(np.array(costs), 1/100*np.ones(100), mode='valid')\n",
    "smoothcosts = meanfilter(costs)\n",
    "plt.plot(np.array(costs))\n",
    "plt.plot(np.array(smoothcosts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from src.utils.clustering_utils import get_cluster_masks\n",
    "from src.features.hdf5_iterator import Hdf5Iterator\n",
    "def sigmoid(x):\n",
    "    return 1/(1+np.exp(-x))\n",
    "\n",
    "if False:\n",
    "    if numsources == 3:\n",
    "        longmixer = SupervisedMixer([libritrain,libritrain,libritrain], shape=(200,257), \n",
    "                                    mix_method='add', diffseed=True, return_key=True)\n",
    "    elif numsources == 2:\n",
    "        longmixer = SupervisedMixer([libritrain,libritrain], shape=(100,257), \n",
    "                                    mix_method='add', diffseed=True, return_key=True)\n",
    "\n",
    "\n",
    "# Check the time\n",
    "tbeg = time.clock()\n",
    "Xtest, Ytest, Itest = longmixer.get_batch(2, out_TF=None)\n",
    "Xin = np.sqrt( abs(Xtest) )\n",
    "Xin = (Xin - Xin.min()) / (Xin.max() - Xin.min())\n",
    "\n",
    "tend = time.clock()\n",
    "print('Supervised feature mixer with 3 libridev sources timed at ', (tend-tbeg), 'sec')\n",
    "\n",
    "Vin, Vcl = sess.run([model.network, model.Vclass], feed_dict={X: abs(Xin), Y:Ytest, I:Idata})\n",
    "masks = get_cluster_masks(Vin, 2)\n",
    "\n",
    "plt.figure(figsize=(12,12)); \n",
    "plt.subplot(121); plt.imshow( masks[:,:,0].T, aspect=.2, cmap='bone' )\n",
    "plt.subplot(122); plt.imshow( Ytest[0,0].T, aspect=.2, cmap='bone' )\n",
    "\n",
    "plt.figure(figsize=(12,12)); \n",
    "plt.subplot(121); plt.imshow( masks[:,:,1].T, aspect=.2, cmap='bone' )\n",
    "plt.subplot(122); plt.imshow( Ytest[0,1].T, aspect=.2, cmap='bone' )\n",
    "\n",
    "if numsources == 3:\n",
    "    plt.figure(figsize=(12,12)); \n",
    "    plt.subplot(121); plt.imshow( masks[:,:,2].T, aspect=.2, cmap='bone' )\n",
    "    plt.subplot(122); plt.imshow( Ytest[0,2].T, aspect=.2, cmap='bone' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.features.spectral_features import reconstruct\n",
    "from IPython.display import Audio\n",
    "from IPython.display import display\n",
    "\n",
    "masks = get_cluster_masks(abs(Vin), 2)\n",
    "masks = masks.transpose(2,0,1)\n",
    "Ytest = (Ytest + 1)/2\n",
    "\n",
    "# Stupid hack, there's a better way to do this\n",
    "mask = masks[0]\n",
    "soundshape = reconstruct( (abs(Xtest[0]) * mask), np.angle(Xtest[0]), 10000, 0.0512, 0.0256 ).shape\n",
    "Xsound = np.zeros( (numsources+1, soundshape[0]) )\n",
    "Ysound = np.zeros( (numsources, soundshape[0]) )\n",
    "\n",
    "Xsound[0] = reconstruct( abs(Xtest[0]), Xtest[0], 10000, 0.0512, 0.0256 )\n",
    "for i, mask in enumerate(masks):\n",
    "    Xsound[i+1] = reconstruct( abs(Xtest[0]) * mask, Xtest[0], 10000, 0.0512, 0.0256 )\n",
    "    Ysound[i] = reconstruct( abs(Xtest[0]) * Ytest[0,i], Xtest[0], 10000, 0.0512, 0.0256 )\n",
    "    \n",
    "    \n",
    "print(\"ORIGINAL\")\n",
    "display(Audio(Xsound[0], rate=10000))\n",
    "print(\"IDEAL MASK 1\")\n",
    "display(Audio(Ysound[0], rate=10000))\n",
    "print(\"PREDICTED MASK 1\")\n",
    "display(Audio(Xsound[1], rate=10000))\n",
    "print(\"IDEAL MASK 2\")\n",
    "display(Audio(Ysound[1], rate=10000))\n",
    "print(\"PREDICTED MASK 2\")\n",
    "display(Audio(Xsound[2], rate=10000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Vcl_30k = sess.run( tf.trainable_variables()[1] )\n",
    "# Vcl_31k = sess.run( tf.trainable_variables()[1] )\n",
    "tf.trainable_variables()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow1.1",
   "language": "python",
   "name": "tf1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
