{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%autoreload 3\n",
    "import os\n",
    "import time\n",
    "from itertools import islice, permutations, product\n",
    "from collections import namedtuple\n",
    "from importlib import reload\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as display\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected, flatten\n",
    "\n",
    "from magnolia.features.mixer import FeatureMixer\n",
    "from magnolia.features.wav_iterator import batcher\n",
    "from magnolia.features.hdf5_iterator import SplitsIterator, Hdf5Iterator\n",
    "import magnolia.features.spectral_features as sf\n",
    "from magnolia.utils.postprocessing import reconstruct\n",
    "reload(sf)\n",
    "from magnolia.utils.tf_utils import scope_decorator as scope\n",
    "from magnolia.dnnseparate.pit import PITModel\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_srcs = 2\n",
    "num_steps = 51\n",
    "num_freq_bins = 257\n",
    "batch_size = 256\n",
    "\n",
    "checkpoint_path = \"/local_data/pcallier/pit_checkpoint\"\n",
    "\n",
    "def moving_average(a, n=3):\n",
    "    ret = np.cumsum(a, dtype=float)\n",
    "    ret[n:] = ret[n:] - ret[:-n]\n",
    "    return ret[n - 1:] / n\n",
    "\n",
    "def scale_spectrogram(spectrogram):\n",
    "    mag_spec = np.abs(spectrogram)\n",
    "    phases = np.unwrap(np.angle(spectrogram))\n",
    "    \n",
    "    mag_spec = np.sqrt(mag_spec)\n",
    "    M = mag_spec.max()\n",
    "    m = mag_spec.min()\n",
    "    \n",
    "    return (mag_spec - m)/(M - m), phases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model_name = \"pit_dnn_{}spkr\".format(num_srcs)\n",
    "model = PITModel('pit-s-dnn', num_srcs, num_steps, num_freq_bins, learning_rate=5e-4)\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "training_session_start = time.time()\n",
    "\n",
    "losses = []\n",
    "val_losses = []\n",
    "weight_changes = []\n",
    "train_times  = []\n",
    "MbStats = namedtuple('MbStats',['i', 'loss', 'time_delta', 'timestamp', 'batch_size'])\n",
    "train_stats = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "librispeech_dev = \"/local_data/teams/magnolia/librispeech/processed_dev-clean.h5\"\n",
    "librispeech_train = \"/local_data/teams/magnolia/librispeech/processed_train-clean-100.h5\"\n",
    "librispeech_test = \"/local_data/teams/magnolia/librispeech/processed_test_clean.h5\"\n",
    "\n",
    "feature_iterators = [SplitsIterator([0.8,0.1,0.1], librispeech_train, \n",
    "                     shape=(num_steps, None), seed=i) for i in range(num_srcs)]\n",
    "mixed_features = FeatureMixer(feature_iterators, shape=(num_steps, None))\n",
    "data_batches = batcher(mixed_features, batch_size)\n",
    "\n",
    "feature_iterators_val_inset = [SplitsIterator([0.8,0.1,0.1], librispeech_train, \n",
    "                     shape=(num_steps, None), seed=i) for i in range(num_srcs)]\n",
    "for i in range(num_srcs):\n",
    "    feature_iterators_val_inset[i].set_split(1)\n",
    "mixed_features_val_inset = FeatureMixer(feature_iterators_val_inset, shape=(num_steps, None))\n",
    "data_batches_val_inset = batcher(mixed_features_val_inset, batch_size)\n",
    "\n",
    "feature_iterators_test_inset = [SplitsIterator([0.8,0.1,0.1], librispeech_train, \n",
    "                     shape=(num_steps, None), seed=i) for i in range(num_srcs)]\n",
    "for i in range(num_srcs):\n",
    "    feature_iterators_test_inset[i].set_split(2)\n",
    "mixed_features_test_inset = FeatureMixer(feature_iterators_test_inset, shape=(num_steps, None))\n",
    "data_batches_test_inset = batcher(mixed_features_test_inset, batch_size)\n",
    "\n",
    "feature_iterators_val_outset = [Hdf5Iterator(librispeech_dev, \n",
    "                     shape=(num_steps, None), seed=i) for i in range(num_srcs)]\n",
    "mixed_features_val_outset = FeatureMixer(feature_iterators_val_outset, shape=(num_steps, None))\n",
    "data_batches_val_outset = batcher(mixed_features_val_outset, batch_size)\n",
    "\n",
    "feature_iterators_test_outset = [Hdf5Iterator(librispeech_test, \n",
    "                     shape=(num_steps, None), seed=i) for i in range(num_srcs)]\n",
    "mixed_features_test_outset = FeatureMixer(feature_iterators_test_outset, shape=(num_steps, None))\n",
    "data_batches_test_outset = batcher(mixed_features_test_outset, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3 speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pdb off\n",
    "prev_first_layer_weights = None\n",
    "prev_weights = None\n",
    "saver = tf.train.Saver()\n",
    "nbatches = 100000 if num_srcs == 3 else 0\n",
    "for i, batch in enumerate(islice(data_batches, nbatches)):\n",
    "    \n",
    "    # Get training data\n",
    "    batch_features, batch_ref1, batch_ref2, batch_ref3 = batch\n",
    "    batch_ref1_norm, batch_ref1_norm_phase = scale_spectrogram(batch_ref1) \n",
    "    batch_ref2_norm, batch_ref2_norm_phase = scale_spectrogram(batch_ref2)\n",
    "    batch_ref3_norm, batch_ref3_norm_phase = scale_spectrogram(batch_ref3)\n",
    "    batch_features_norm, batch_features_norm_phase = scale_spectrogram(batch_features)\n",
    "    \n",
    "    # Do optimization\n",
    "    data = { \n",
    "        model.X_in: batch_features_norm,\n",
    "        model.y_in: np.stack((batch_ref1_norm, batch_ref2_norm, batch_ref3_norm), axis=1)\n",
    "        }\n",
    "    sess.run(model.optimize, feed_dict=data)\n",
    "    loss = sess.run(model.loss, feed_dict=data)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    # get validation statistics\n",
    "    val_batch = next(val_batches)\n",
    "    (val_feat_norm, _), (val_ref1_norm, _), (val_ref2_norm, _), (val_ref3_norm, _) = \\\n",
    "        [scale_spectrogram(x) for x in val_batch]\n",
    "    val_data = { model.X_in: val_feat_norm, \n",
    "                model.y_in: np.stack((val_ref1_norm, val_ref2_norm, val_ref3_norm), axis=1)}\n",
    "    val_loss = sess.run(model.loss, feed_dict=val_data)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Collect predictions, gradient statistics, masks\n",
    "    prediction = sess.run(model.predict, data)\n",
    "    all_weights = sess.run(tf.concat([tf.reshape(var, [-1]) for var in tf.trainable_variables()], axis=0), data)\n",
    "    if prev_weights is not None:\n",
    "        weight_changes.append(np.linalg.norm(all_weights-prev_weights))\n",
    "    prev_weights = all_weights\n",
    "    if model.mask is not None:\n",
    "        masks = sess.run(model.mask, data)\n",
    "    else:\n",
    "        masks = None\n",
    "    \n",
    "    # Checkpointing\n",
    "    if len(losses) > 0 and len(losses) % 1000 == 0:\n",
    "#     if True:\n",
    "        saver.save(sess, os.path.join(checkpoint_path, \"{model}_{batch}_{stamp}.ckpt\".format(\n",
    "            model=model_name,\n",
    "            batch=len(losses),\n",
    "            stamp=hex(hash(time.time()))[2:8]\n",
    "        )))\n",
    "    \n",
    "    # Plotting\n",
    "    if len(losses) > 0 and len(losses) % 10 == 0:\n",
    "#     if True:\n",
    "        orig = reconstruct(batch_features[0], batch_features[0], fs=10000, window_size=None, step_size=0.0256)\n",
    "        recon_a = reconstruct(prediction[0,0]**2, batch_features[0], fs=10000, window_size=None, step_size=0.0256)\n",
    "        recon_b = reconstruct(prediction[0,1]**2, batch_features[0], fs=10000, window_size=None, step_size=0.0256)\n",
    "        recon_c = reconstruct(prediction[0,2]**2, batch_features[0], fs=10000, window_size=None, step_size=0.0256)\n",
    "        orig_a = reconstruct(batch_ref1[0], batch_ref1[0], fs=10000, window_size=None, step_size=0.0256)\n",
    "        orig_b = reconstruct(batch_ref2[0], batch_ref2[0], fs=10000, window_size=None, step_size=0.0256)\n",
    "        orig_c = reconstruct(batch_ref3[0], batch_ref3[0], fs=10000, window_size=None, step_size=0.0256)\n",
    "        \n",
    "        display.clear_output(wait=True)\n",
    "        plt.figure(figsize=(8,9))\n",
    "        plt.subplot(3,4,1); plt.imshow(np.sqrt(prediction[0,0].T), cmap='bone', origin='lower', aspect=0.25)\n",
    "        plt.subplot(3,4,2); plt.imshow(np.sqrt(prediction[0,1].T), cmap='bone', origin='lower', aspect=0.25)\n",
    "        plt.subplot(3,4,3); plt.imshow(np.sqrt(prediction[0,2].T), cmap='bone', origin='lower', aspect=0.25)\n",
    "        plt.subplot(3,4,4); plt.imshow(batch_features_norm[0].T, cmap='bone', origin='lower', aspect=0.25)\n",
    "        if masks is not None:\n",
    "            plt.subplot(3,4,5); plt.imshow(masks[0,0].T, cmap='bone', origin='lower', aspect=0.25, vmin=0, vmax=1)\n",
    "            plt.subplot(3,4,6); plt.imshow(masks[1,0].T, cmap='bone', origin='lower', aspect=0.25, vmin=0, vmax=1)\n",
    "            plt.subplot(3,4,7); plt.imshow(masks[2,0].T, cmap='bone', origin='lower', aspect=0.25, vmin=0, vmax=1)\n",
    "            plt.subplot(3,4,8); plt.imshow(masks[2,1].T, cmap='bone', origin='lower', aspect=0.25, vmin=0, vmax=1)\n",
    "            plt.title(\"Next example\")\n",
    "            \n",
    "        plt.subplot(3,4,9); plt.imshow(batch_ref1_norm[0].T, cmap='bone', origin='lower', aspect=0.25)\n",
    "        plt.subplot(3,4,10); plt.imshow(batch_ref2_norm[0].T, cmap='bone', origin='lower', aspect=0.25)\n",
    "        plt.subplot(3,4,11); plt.imshow(batch_ref3_norm[0].T, cmap='bone', origin='lower', aspect=0.25)\n",
    "        plt.subplot(3,4,12); plt.imshow(batch_features_norm[1].T, cmap='bone', origin='lower', aspect=0.25)\n",
    "        \n",
    "        \n",
    "        \n",
    "        smooth_factor = len(losses) // 10\n",
    "        if smooth_factor <= 5:\n",
    "            avg_losses = losses\n",
    "            avg_val = val_losses\n",
    "            avg_chg = weight_changes\n",
    "        else:\n",
    "            avg_losses = moving_average(losses, smooth_factor)\n",
    "            avg_val = moving_average(losses)\n",
    "            avg_chg = moving_average(weight_changes, smooth_factor)\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(avg_losses)\n",
    "        plt.plot(avg_val, color='red')\n",
    "        plt.title(\"Losses\")\n",
    "        \n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(avg_chg)\n",
    "        plt.title(\"Norm of weight changes from previous minibatch\")\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow(sess.run(model.logits, data), cmap='afmhot', aspect=3)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        print(\"Mix\")\n",
    "        display.display(display.Audio(orig, rate=10000))\n",
    "        print(\"Recon A\")\n",
    "        display.display(display.Audio(recon_a, rate=10000))\n",
    "        print(\"Recon B\")\n",
    "        display.display(display.Audio(recon_b, rate=10000))\n",
    "        print(\"Recon C\")\n",
    "        display.display(display.Audio(recon_c, rate=10000))\n",
    "        print(\"Originals\")\n",
    "        display.display(display.Audio(orig_a, rate=10000))\n",
    "        display.display(display.Audio(orig_b, rate=10000))\n",
    "        display.display(display.Audio(orig_c, rate=10000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%pdb off\n",
    "prev_first_layer_weights = None\n",
    "prev_weights = None\n",
    "saver = tf.train.Saver()\n",
    "nbatches = 100000 if num_srcs == 2 else 0\n",
    "for i, batch in enumerate(islice(data_batches, nbatches)):\n",
    "    \n",
    "    # Get training data\n",
    "    batch_features, batch_ref1, batch_ref2 = batch\n",
    "    batch_ref1_norm, batch_ref1_norm_phase = scale_spectrogram(batch_ref1) \n",
    "    batch_ref2_norm, batch_ref2_norm_phase = scale_spectrogram(batch_ref2)\n",
    "    batch_features_norm, batch_features_norm_phase = scale_spectrogram(batch_features)\n",
    "    \n",
    "    # Do optimization\n",
    "    data = { \n",
    "        model.X_in: batch_features_norm,\n",
    "        model.y_in: np.stack((batch_ref1_norm, batch_ref2_norm), axis=1)\n",
    "        }\n",
    "    train_start = time.time()\n",
    "    sess.run(model.optimize, feed_dict=data)\n",
    "    train_end = time.time()\n",
    "    loss = sess.run(model.loss, feed_dict=data)\n",
    "    losses.append(loss)\n",
    "    \n",
    "    minibatch_stats = MbStats(len(losses), loss, train_end - train_start, train_start, batch_size)\n",
    "    train_stats.append(minibatch_stats)\n",
    "    \n",
    "    # get validation statistics (in-set)\n",
    "    val_batch = next(data_batches_val_inset)\n",
    "    (val_feat_norm, _), (val_ref1_norm, _), (val_ref2_norm, _) = \\\n",
    "        [scale_spectrogram(x) for x in val_batch]\n",
    "    val_data = { model.X_in: val_feat_norm, \n",
    "                model.y_in: np.stack((val_ref1_norm, val_ref2_norm), axis=1)}\n",
    "    val_loss = sess.run(model.loss, feed_dict=val_data)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    # Collect predictions, gradient statistics, masks\n",
    "    prediction = sess.run(model.predict, data)\n",
    "    all_weights = sess.run(tf.concat([tf.reshape(var, [-1]) for var in tf.trainable_variables()], axis=0), data)\n",
    "    if prev_weights is not None:\n",
    "        weight_changes.append(np.linalg.norm(all_weights-prev_weights))\n",
    "    prev_weights = all_weights\n",
    "    if model.mask is not None:\n",
    "        masks = sess.run(model.mask, data)\n",
    "    else:\n",
    "        masks = None\n",
    "        \n",
    "    \n",
    "    \n",
    "    # Checkpointing\n",
    "    if len(losses) > 0 and len(losses) % 1000 == 0:\n",
    "#     if True:\n",
    "        saver.save(sess, os.path.join(checkpoint_path, \"{model}_{batch}_{stamp}.ckpt\".format(\n",
    "            model=model_name,\n",
    "            batch=len(losses),\n",
    "            stamp=hex(hash(time.time()))[2:8]\n",
    "        )))\n",
    "\n",
    "    \n",
    "    # Plotting\n",
    "    if len(losses) > 0 and len(losses) % 10 == 0:\n",
    "#     if True:\n",
    "        with open(os.path.join(checkpoint_path, \"{model}_stats.txt\".format(\n",
    "            model=model_name\n",
    "        )), \"a\") as f:\n",
    "            for line in train_stats:\n",
    "                f.write('\\t'.join([str(x) for x in line]))\n",
    "                f.write('\\n')\n",
    "            train_stats = []\n",
    "            \n",
    "        orig = reconstruct(batch_features[0], batch_features[0], fs=10000, window_size=None, step_size=0.0256)\n",
    "        recon_a = reconstruct(prediction[0,0]**2, batch_features[0], fs=10000, window_size=None, step_size=0.0256)\n",
    "        recon_b = reconstruct(prediction[0,1]**2, batch_features[0], fs=10000, window_size=None, step_size=0.0256)\n",
    "        orig_a = reconstruct(batch_ref1[0], batch_ref1[0], fs=10000, window_size=None, step_size=0.0256)\n",
    "        orig_b = reconstruct(batch_ref2[0], batch_ref2[0], fs=10000, window_size=None, step_size=0.0256)\n",
    "        \n",
    "        display.clear_output(wait=True)\n",
    "        plt.figure(figsize=(8,9))\n",
    "        plt.subplot(3,4,1); plt.imshow(np.sqrt(prediction[0,0].T), cmap='bone', origin='lower', aspect=0.25)\n",
    "        plt.subplot(3,4,2); plt.imshow(np.sqrt(prediction[0,1].T), cmap='bone', origin='lower', aspect=0.25)\n",
    "        plt.subplot(3,4,4); plt.imshow(batch_features_norm[0].T, cmap='bone', origin='lower', aspect=0.25)\n",
    "        if masks is not None:\n",
    "            plt.subplot(3,4,5); plt.imshow(masks[0,0].T, cmap='bone', origin='lower', aspect=0.25, vmin=0, vmax=1)\n",
    "            plt.subplot(3,4,6); plt.imshow(masks[1,0].T, cmap='bone', origin='lower', aspect=0.25, vmin=0, vmax=1)\n",
    "            \n",
    "        plt.subplot(3,4,9); plt.imshow(batch_ref1_norm[0].T, cmap='bone', origin='lower', aspect=0.25)\n",
    "        plt.subplot(3,4,10); plt.imshow(batch_ref2_norm[0].T, cmap='bone', origin='lower', aspect=0.25)\n",
    "        \n",
    "        \n",
    "        smooth_factor = len(losses) // 10\n",
    "        if smooth_factor <= 5:\n",
    "            avg_losses = losses\n",
    "            avg_val = val_losses\n",
    "            avg_chg = weight_changes\n",
    "        else:\n",
    "            avg_losses = moving_average(losses, smooth_factor)\n",
    "            avg_val = moving_average(val_losses, smooth_factor)\n",
    "            avg_chg = moving_average(weight_changes, smooth_factor)\n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(avg_losses)\n",
    "        plt.plot(avg_val, color='red')\n",
    "        plt.title(\"Losses\")\n",
    "        \n",
    "        plt.figure(figsize=(8,5))\n",
    "        plt.plot(avg_chg)\n",
    "        plt.title(\"Norm of weight changes from previous minibatch\")\n",
    "        \n",
    "        plt.figure()\n",
    "        plt.imshow(sess.run(model.logits, data), cmap='afmhot', aspect=3)\n",
    "        plt.colorbar()\n",
    "        plt.show()\n",
    "        print(\"Mix\")\n",
    "        display.display(display.Audio(orig, rate=10000))\n",
    "        print(\"Recon A\")\n",
    "        display.display(display.Audio(recon_a, rate=10000))\n",
    "        print(\"Recon B\")\n",
    "        display.display(display.Audio(recon_b, rate=10000))\n",
    "\n",
    "        print(\"Originals\")\n",
    "        display.display(display.Audio(orig_a, rate=10000))\n",
    "        display.display(display.Audio(orig_b, rate=10000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "[str( x )for x in minibatch_stats]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [magnolia3]",
   "language": "python",
   "name": "Python [magnolia3]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
