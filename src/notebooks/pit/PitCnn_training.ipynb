{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training PIT-S-CNN\n",
    "## Permutation-invariant training for monaural source separation\n",
    "\n",
    "This notebook contains a detailed example of how to train one of the models we replicated, specifically the CNN-based permutation-invariant training approach.  Filepaths to load training data must be filled in to run this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import sys\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Plotting imports\n",
    "import IPython\n",
    "from IPython.display import Audio\n",
    "from matplotlib import pyplot as plt\n",
    "fig_size = [0,0]\n",
    "fig_size[0] = 8\n",
    "fig_size[1] = 4\n",
    "plt.rcParams[\"figure.figsize\"] = fig_size\n",
    "\n",
    "# Import Lab41's separation model\n",
    "from magnolia.dnnseparate.pit import PITModel\n",
    "\n",
    "# Import utilities for using the model\n",
    "# from magnolia.utils.clustering_utils import clustering_separate, get_cluster_masks, process_signal\n",
    "from magnolia.iterate.supervised_iterator import SupervisedIterator, SupervisedMixer\n",
    "from magnolia.iterate.hdf5_iterator import SplitsIterator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "libritrain = \"** path to LibriSpeech train hdf5 **\"\n",
    "female_speakers = '** path to list of female speakers in train set (available in repo) **'\n",
    "male_speakers = '** path to list of male speakers in train set (in repo) **'\n",
    "checkpoint_path = \"** path to save checkpoint at **\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "    numsources  : Number of sources used in training mixes\n",
    "    batchsize   : Number of examples per batch used in training\n",
    "    fft_size    : Number of FFT points used for STFT\n",
    "    datashape   : (Time, Frequency) shape of the examples within each batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "numsources = 2\n",
    "batchsize = 256\n",
    "fft_size = 512\n",
    "datashape = (40, fft_size//2 + 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up data I/O\n",
    "\n",
    "For training, only the training dataset is needed.  The other two datasets can be used for evaluation.  The (training set, or in set) speaker keys have been separated according to speaker gender."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open(female_speakers,'r') as speakers:\n",
    "    keys = speakers.read().splitlines()\n",
    "    speaker_keys = keys[:]\n",
    "    in_set_F = keys[:]\n",
    "    \n",
    "with open(male_speakers,'r') as speakers:\n",
    "    keys = speakers.read().splitlines()\n",
    "    speaker_keys += keys\n",
    "    in_set_M = keys[:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an mixer that iterates over examples from the training set. \n",
    "\n",
    "SplitsIterator handles (deterministically) splitting the training set into three partitions.  80% of the training data is used to train the model, 10% is used to evaluate the training progress on unseen examples, and the last 10% is reserved to evaluate the performance of the model on unseen examples from speakers in the training set.\n",
    "\n",
    "SupervisedMixer handles the mixing of training examples. It outputs the model input (X), the output labels (Y) and the speakerIDs (I) of the speakers who are loudest in each time frequency bin.  Y must be reshaped and transposed so that it has shape (batchsize,time,frequency,numspeakers).\n",
    "\n",
    "Scaling of the mixtures to create input batches for the model is done here as well.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Create the splits iterator\n",
    "siterator = SplitsIterator([0.8,0.1,0.1], libritrain, speaker_keys=speaker_keys, shape=datashape, return_key=True)\n",
    "siterator.set_split(0)\n",
    "\n",
    "# Create the data mixer\n",
    "mixer = SupervisedMixer([siterator,siterator], shape=datashape, \n",
    "                        mix_method='add', diffseed=True)\n",
    "\n",
    "# Generate a sample batch of training data\n",
    "X, Y, I = mixer.get_batch(batchsize, out_TF=None)\n",
    "Y = Y.reshape(batchsize,2,datashape[0],datashape[1])\n",
    "#Y = Y.transpose([0,2,3,1])\n",
    "\n",
    "# Scale the model inputs\n",
    "X = np.sqrt(X)\n",
    "X = (X - X.min())/(X.max() - X.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate some validation data\n",
    "To generate a batch from the validation split of the training dataset, the splits iterator can have the split set to the validation split and the mixer can be used as before.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the current split to the validation split\n",
    "siterator.set_split(1)\n",
    "\n",
    "# Generate a batch of validation data\n",
    "Xdv, Ydv, Idv = mixer.get_batch(batchsize, out_TF=None)\n",
    "Ydv = Ydv.reshape(batchsize,numsources,datashape[0],datashape[1])\n",
    "# Ydv = Ydv.transpose(0,2,3,1)\n",
    "\n",
    "# Scale the model inputs\n",
    "Xinv = np.sqrt(Xdv)\n",
    "Xinv = (Xinv - Xinv.min())/(Xinv.max() - Xinv.min())\n",
    "\n",
    "# Set the split back to the training split\n",
    "siterator.set_split(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create an instance of Lab41's model\n",
    "\n",
    "Here an untrained model instance is created, and its variables are initialized. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tf.reset_default_graph()\n",
    "\n",
    "model = PITModel(method='pit-s-cnn', num_steps=datashape[0], num_freq_bins=datashape[1], num_srcs=numsources)\n",
    "\n",
    "config = tf.ConfigProto()\n",
    "config.allow_soft_placement = True\n",
    "config.gpu_options.allow_growth = True\n",
    "\n",
    "sess = tf.Session(config=config)\n",
    "sess.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables needed to track the training progress of the model\n",
    "\n",
    "During training, the number of iterations (number of processed batches) is tracked, along with the mean cost on examples from the training data and from the validation data.  The last iteration that the model was saved on can also be tracked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "iterations = []\n",
    "costs = []\n",
    "\n",
    "t_costs = []\n",
    "v_costs = []\n",
    "\n",
    "last_saved = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training loop\n",
    "\n",
    "Here the model is iteratively trained on batches generated by the mixer.  The model is saved every time the validation cost reaches a new minimum value.  The training can be configured to stop if the model has not been saved after a specified number of iterations have elapsed since the previous save.  Plots of the training cost and the validation set are created as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Number of iterations to train for (should be large)\n",
    "num_iterations = 1000000\n",
    "# Threshold for stopping if the model hasn't improved for this many consecutive iterations\n",
    "stop_threshold = 10000\n",
    "\n",
    "# Find the number of iterations already elapsed (Useful for resuming training)\n",
    "if len(iterations) == 0:\n",
    "    start = 0\n",
    "else:\n",
    "    start = iterations[-1]\n",
    "\n",
    "# Ensure that the iterator is set to iterate over the training split\n",
    "siterator.set_split(0)\n",
    "\n",
    "# Iterate over training batches\n",
    "for i in range(num_iterations):\n",
    "    \n",
    "    # Generate a batch of training data\n",
    "    Xdata, Ydata, Idata = mixer.get_batch(batchsize, out_TF=None)\n",
    "    Ydata = Ydata.reshape(batchsize,2,datashape[0],datashape[1])\n",
    "#     Ydata = Ydata.transpose(0,2,3,1)\n",
    "    \n",
    "    # Scale the inputs\n",
    "    Xin = np.sqrt(np.abs(Xdata))\n",
    "    Xin = (Xin - Xin.min())/(Xin.max() - Xin.min())\n",
    "    data = { model.X_in: Xin,\n",
    "             model.y_in: Ydata}\n",
    "    \n",
    "    # Train the model on one batch and get the cost\n",
    "    sess.run(model.optimize, data)\n",
    "    cost = sess.run(model.loss, data)\n",
    "#     c = model.train_on_batch(Xin,Ydata,Idata)\n",
    "\n",
    "    # Store the training cost\n",
    "    costs.append(cost)\n",
    "    \n",
    "    # Every 10 batches, evaluate the model on the validation data and plot the cost curves\n",
    "    if (i+1) % 10 == 0:\n",
    "        IPython.display.clear_output(wait=True)\n",
    "        \n",
    "        val_data = { model.X_in: Xinv,\n",
    "                     model.y_in: Ydv }\n",
    "        \n",
    "        # Get the cost on the validation batch\n",
    "        c_v = sess.run(model.loss, val_data)\n",
    "        \n",
    "        # Check if the validation cost is below the minimum validation cost, and if so, save it.\n",
    "        if len(iterations) > 0 and c_v < min(v_costs):\n",
    "            print(\"Saving the model because c_v is\", min(v_costs) - c_v, \"below the old min.\")\n",
    "            \n",
    "            # Save the model to the specified path\n",
    "            model.save(checkpoint_path, sess)\n",
    "            \n",
    "            # Record the iteraion that the model was last saved on\n",
    "            last_saved = iterations[-1]\n",
    "        \n",
    "        # Store the training cost and the validation cost\n",
    "        t_costs.append(np.mean(costs))\n",
    "        v_costs.append(c_v)\n",
    "        \n",
    "        # Store the current iteration number\n",
    "        iterations.append(i + 1 + start)\n",
    "        \n",
    "        # Compute scale quantities for plotting\n",
    "        length = len(iterations)\n",
    "        cutoff = int(0.5*length)\n",
    "        lowline = [min(v_costs)]*len(iterations)\n",
    "        \n",
    "        # Generate the plots and show them\n",
    "        f, (ax1, ax2) = plt.subplots(2,1)\n",
    "        \n",
    "        ax1.plot(iterations,t_costs)\n",
    "        ax1.plot(iterations,v_costs)\n",
    "        ax1.plot(iterations,lowline)\n",
    "        \n",
    "        y_u = max(max(t_costs[cutoff:]),max(v_costs[cutoff:]))\n",
    "        y_l = min(min(t_costs[cutoff:]),min(v_costs[cutoff:]))\n",
    "        \n",
    "        ax2.set_ylim(y_l,y_u)\n",
    "        \n",
    "        ax2.plot(iterations[cutoff:], t_costs[cutoff:])\n",
    "        ax2.plot(iterations[cutoff:], v_costs[cutoff:])\n",
    "        ax2.plot(iterations[cutoff:], lowline[cutoff:])\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"Cost on iteration\", iterations[-1], \"is\", c_v)\n",
    "        print(\"Last saved\",iterations[-1]-last_saved,\"iterations ago.\")\n",
    "        \n",
    "        # Reset the cost over the last 10 iterations\n",
    "        costs = []\n",
    "        \n",
    "        # Stop training if the number of iterations since the last save point exceeds the threshold\n",
    "        if iterations[-1]-last_saved > stop_threshold:\n",
    "            print(\"Done!\")\n",
    "            break"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "tensorflow1.1",
   "language": "python",
   "name": "tf1.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
